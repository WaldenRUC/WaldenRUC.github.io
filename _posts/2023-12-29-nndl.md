---
title: 'Neural Network and Deep Learning'
date: 2023-12-29
permalink: /posts/2023/04/nndl/
tags:
  - Pytorch
  - Python
  - DeepLearning
---

# 神经网络深度学习笔记

本文主要归纳神经网络深度学习中的易错、容易遗忘的知识点。

## Chapter 1. 绪论

机器学习：将数据表示为一组特征（比如人的身高、体重等），选取特征是一件非常耗时的事情。

表示学习：将输入信息转换为**有效的特征**（或者称为表示）。表示学习的核心：解决语义鸿沟问题，即**底层的特征**（比如图像的像素、一个句子里的分词）和**高层的语义信息**（比如两张图片的文本标签、句子的情感标签）之间的鸿沟。

局部表示：构建词表，用one-hot向量表示：对于新的标签只能扩展向量维度；无法表示相似度（互相正交）

表示学习的关键是构建具有一定深度的**多层次特征表示**

深度学习的目的：从数据中自动学习到有效的特征表示，这些学习到的表示可以替代人工设计的特征。

贡献度分配问题：一个系统中不同的组件或参数对最终系统输出结果的贡献或影响。

端到端学习：不分模块或分阶段训练，而是直接优化任务的总体目标，不需要明确地给出不同模块或阶段的功能。


## Chapter 2. 机器学习概述

特征也可以被称为**属性**，样本由属性和标签构成。

交叉熵损失：用于多分类问题：给定一个样本独热标签和多分类预测的条件概率分布。损失为【标签*预测的对数】之和然后取负值。

Hinge损失：对于标签为[1, -1]的任务，损失函数为max{0, 1-标签*预测值}

经验风险最小化准则：优化使得训练集上的损失最小，但会产生过拟合问题。

结构风险最小化准则：引入参数的正则化（引入参数的先验分布，使其不完全依赖训练数据）

验证集（开发集）：在每次迭代时，将新得到的模型在验证集上测试，如果损失不下降，就提前停止（early stop）

批量梯度下降法：将**整个训练集**的样本损失函数的梯度累计起来计算，开销很大

随机梯度下降法：每次只用一个样本的损失函数梯度计算

小批量梯度下降法：在每一步迭代时，选取K个样本的子集，计算他们**损失函数的梯度的平均值**

参数估计方法：经验风险最小化、结构风险最小化、最大似然估计、最大后验估计。

1. 经验风险最小化：使用平方损失函数衡量真实标签与预测标签之间的差异。

求解风险函数时，令风险函数关于权重$w$的导数为0，则可解得最优参数为$w^* = (XX^T)^{-1}Xy$

其中，y为标签向量，X为样本矩阵（每列为单个样本与1拼接）

2. 结构风险最小化：$(XX^T)^{-1}$的值可能无法准确计算，因此添加常数单位阵，使得修改后的矩阵满秩：
$$
w^* = (XX^T + \lambda I)^{-1} Xy
$$



## Chapter 3. 线性模型

## Chapter 4. 前馈神经网络

## Chapter 5. 卷积神经网络

## Chapter 6. 循环神经网络

## Chapter 7. 网络优化与正则化

## Chapter 8. 注意力机制与外部记忆

## Chapter 9. 无监督学习

## Chapter 10. 模型独立的学习方式

## Chapter 11. 概率图模型

## Chapter 12. 深度信念网络

## Chapter 13. 深度生成模型

## Chapter 14. 深度强化学习

## Chapter 15. 序列生成模型

