---
title: 'Neural Network and Deep Learning'
date: 2023-12-29
permalink: /posts/2023/04/nndl/
tags:
  - Pytorch
  - Python
  - DeepLearning
---

# 神经网络深度学习笔记

本文主要归纳神经网络深度学习中的易错、容易遗忘的知识点。

## Chapter 1. 绪论

机器学习：将数据表示为一组特征（比如人的身高、体重等），选取特征是一件非常耗时的事情。

表示学习：将输入信息转换为**有效的特征**（或者称为表示）。表示学习的核心：解决语义鸿沟问题，即**底层的特征**（比如图像的像素、一个句子里的分词）和**高层的语义信息**（比如两张图片的文本标签、句子的情感标签）之间的鸿沟。

局部表示：构建词表，用one-hot向量表示：对于新的标签只能扩展向量维度；无法表示相似度（互相正交）

表示学习的关键是构建具有一定深度的**多层次特征表示**

深度学习的目的：从数据中自动学习到有效的特征表示，这些学习到的表示可以替代人工设计的特征。

贡献度分配问题：一个系统中不同的组件或参数对最终系统输出结果的贡献或影响。

端到端学习：不分模块或分阶段训练，而是直接优化任务的总体目标，不需要明确地给出不同模块或阶段的功能。


## Chapter 2. 机器学习概述

特征也可以被称为**属性**，样本由属性和标签构成。

交叉熵损失：用于多分类问题：给定一个样本独热标签和多分类预测的条件概率分布。损失为【
$$
-\sum y_i  \text{log} \hat{y_i}
$$
】。

Hinge损失：对于标签为
$$
\{1, -1\}
$$
的任务，损失函数为
$$
max\{0, 1-y*\hat{y}\}
$$

经验风险最小化准则：优化使得训练集上的损失最小，但会产生过拟合问题。

结构风险最小化准则：引入参数的正则化（引入参数的先验分布，使其不完全依赖训练数据）

验证集（开发集）：在每次迭代时，将新得到的模型在验证集上测试，如果损失不下降，就提前停止（early stop）

批量梯度下降法：将**整个训练集**的样本损失函数的梯度累计起来计算，开销很大

随机梯度下降法：每次只用一个样本的损失函数梯度计算

小批量梯度下降法：在每一步迭代时，选取K个样本的子集，计算他们**损失函数的梯度的平均值**

参数估计方法：经验风险最小化、结构风险最小化、最大似然估计、最大后验估计。

- 经验风险最小化：使用平方损失函数衡量真实标签与预测标签之间的差异。

求解风险函数时，令风险函数关于权重$w$的导数为0，则可解得最优参数为$w^* = (XX^T)^{-1}Xy$

其中，y为标签向量，X为样本矩阵（每列为单个样本与1拼接）

- 结构风险最小化：$(XX^T)^{-1}$的值可能无法准确计算，因此采用岭回归，添加常数单位阵，使得修改后的矩阵满秩：
$$
w^* = (XX^T + \lambda I)^{-1} Xy
$$

岭回归即$l_2$正则化。

- 最大似然估计：建模条件概率
$$
p(y|\mathbf{x})
$$
服从某个未知分布。缺点：数据量较少的时候会发生过拟合。

- 最大后验估计：给参数加上一些先验知识：假设参数$\mathbf{w}$服从一个先验分布$p(\mathbf{w}; v)$，然后可以计算后验分布。这样可以得到一个最优的参数值（点估计）。

偏差-方差分解：
- 偏差：预测中心与真实标签之间的差异：
$$
\mathbb{E}_{D}[f_{D}(\mathbf{x})] - f^*(\mathbf{x})
$$
- 方差：预测标签集合与预测中心的差异：
$$
f_{D}(\mathbf{x}) - \mathbb{E}_{D}[f_{D}(\mathbf{x})]
$$

最小化期望错误 = 最小化偏差与方差之和

偏差较大，则说明模型在训练集上的效果不佳，欠拟合。
模型复杂度提升，导致偏差减小但方差增大，从而导致过拟合。

监督学习：分为回归问题、分类问题、结构化学习问题。

词袋模型(Bag-of-Words)：编码句子，将句子中出现的子词在向量中标记为1，如$[1, 1, 0, 1]^T$.

$l_1$正则化容易使得特征稀疏（特征被优化为0），正则化项：
$$\lambda \sum |\theta_i|$$

假负例：模型的预测为【负】，模型的预测结果是错误的【假的】，即真实标签为【真】。

一般不关注真负例。

F1值：精确率和召回率的调和平均。

宏平均：**每一类**的指标的平均值

奥卡姆剃刀：如果两个模型性能相近，我们应该选择**更简单的模型**。引入参数正则化限制模型能力，避免过拟合。

## Chapter 3. 线性模型





## Chapter 4. 前馈神经网络

## Chapter 5. 卷积神经网络

## Chapter 6. 循环神经网络

## Chapter 7. 网络优化与正则化

## Chapter 8. 注意力机制与外部记忆

## Chapter 9. 无监督学习

## Chapter 10. 模型独立的学习方式

## Chapter 11. 概率图模型

## Chapter 12. 深度信念网络

## Chapter 13. 深度生成模型

## Chapter 14. 深度强化学习

## Chapter 15. 序列生成模型

