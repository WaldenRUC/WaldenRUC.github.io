---
title: 'Reinforcement Learning'
date: 2023-04-17
permalink: /posts/2023/04/rl/
tags:
  - Pytorch
  - Python
---

# 强化学习笔记

本文主要记录Reinforcement Learning中可能经常被遗忘的知识点。主要内容参考[easy-rl](https://github.com/datawhalechina/easy-rl)仓库

## Chapter 1. 强化学习基础

#### 强化学习和监督学习的区别

监督学习的输入数据是**没有关联的**，且需要**提供正确的标签**，以修正自己的预测。
强化学习不满足这两个假设。例如雅达利打砖块游戏，模型得到的连续两帧的输入有非常强的连续性，且游戏并没有告诉模型哪个动作是正确的。即使模型预测出了下一步应该把木板往哪个方向移动，这个决策导致的游戏结果也不会立即反馈给模型。
强化学习任务的最终奖励在多步动作之后才能观察到。

强化学习的目标就是让模型（智能体）在这种环境中学习：智能体的动作会影响它随后得到的数据。目标：让智能体的动作稳定地提升。

预演（rollout）：从当前帧对动作采样，生成很多局游戏。每个游戏（可称为**回合episode**或者**试验trial**，可以看成一个轨迹（trajectory）:
$$
\tau = (s_0, a_0, s_1, a_1, \cdots)
$$
其中，$s_i$为第$i$帧，$a_i$为第$i$帧时智能体采取的动作。
为了训练模型，我们可以用**观测序列**和**最终奖励**。
**奖励**：标量的反馈信号，模型的目的是最大化期望累计奖励(expected cumulative reward)，奖励包含游戏分数、股票收益等。

标准强化学习：手工特征提取+分类
深度强化学习：端到端特征提取+分类

状态和观测：状态是完整的描述（用角度和速度表示一个机器人的状态；用RGB表示一个视觉的观测）。
历史是观测、动作、奖励的序列。

动作空间(action space)：智能体的决策集合。

策略(policy)：将输入的状态映射到动作的函数。

随机性策略(stochastic policy)：$\pi$函数，输入为状态$s$，输出为概率$a$：
$$
\pi (a|s) = p(a_t = a|s_t = s)
$$
每一种动作都对应一个概率值。随机性策略按照概率对动作采样，得到智能体将采取的动作。

确定性策略(deterministic policy)：智能体直接采取最有可能的动作:
$$
a^* = \text{argmax}_a \pi (a|s)
$$

价值函数：评估状态的好坏，对未来奖励的预测。
Q价值函数（Q函数）：包含状态和动作，反映我们在使用策略$\pi$时，得到多少奖励: $ Q_{\pi}(s, a) $

模型：依据当前的状态和动作决定下一步的状态。

基于策略的强化学习(policy-based RL)：学习环境，在每个状态，都有一个最佳策略。策略梯度属于此类方法。
基于价值的强化学习(value-based RL)：每个状态会返回一个价值（比如迷宫问题中，每个格子状态的价值就是离终点的最短步数）。根据未来状态的价值，选择动作。Q-learning、Sarsa属于此类方法。

演员-评论员智能体(actor-critic agent): 同时学习策略和价值函数，并通过交互得到最佳动作。

model-based:  学习**状态转移**来采取动作；
model-free：学习价值函数和策略函数来决策（没有估计状态的转移、也没有得到环境的具体转移变量），免模型方法不需要对环境进行建模，直接与真实环境进行交互即可

强化学习任务的定义：马尔科夫过程($S, A, P, R$)：状态集合、动作集合、状态转移函数、奖励函数

如果智能体能根据当前的状态和采取的动作得知下一个时间的状态和奖励($P(s_{t+1} | s_t, a_t), R(s_t, a_t)$)，则称为有模型强化学习。
难点：状态转移函数、奖励函数很难估计；环境状态很可能未知。
model-based比model-free多出一个步骤：**对真实环境建模**。

目前的RL研究的默认假设：环境是静态的、可描述的，智能体的状态是离散的、可观察的。不需要评估状态转移函数和奖励函数，可直接采用免模型强化学习。

探索：探索环境、尝试不同动作来得到最佳策略；
利用：不去尝试新动作、采取已知的带来很大奖励的动作。

**多臂赌博机**：赌徒在投入一个硬币后可以按下其中一个摇臂，每个摇臂有一定概率吐出硬币，但赌徒不知道这个概率。
目标：通过一定策略最大化奖励。
仅探索法：尝试每个摇臂，以平均吐币概率作为奖励期望的估计。
仅利用法：按下目前最优的摇臂（平均奖励最大的）。
尝试次数有限，导致了探索-利用窘境(exploration-exploitation dilemma)

用python的gym框架可以实现一个简单的强化学习框架：
```python
import gym 
env = gym.make("Taxi-v3")   # 取出环境
observation = env.reset()   # 初始化环境
agent = load_agent()
for step in range(100):
    action = agent(observation) 
    observation, reward, done, info = env.step(action)              # 执行环境
```
环境提交一个智能体动作后，返回四个信息：
- observation: 状态，屏幕像素值等等；
- reward: 奖励值，动作提交之后能够获得的奖励值；
- done: 游戏是否已经完成；
- info: 用于诊断和调试的信息

在gym中，可以获取观测空间、动作空间等信息：
```python
import gym
env = gym.make('MountainCar-v0')    # 取出环境
print('观测空间 = {}'.format(env.observation_space))
print('动作空间 = {}'.format(env.action_space))
print('观测范围 = {} ~ {}'.format(env.observation_space.low,
        env.observation_space.high))
print('动作数 = {}'.format(env.action_space.n))
```
输出：
```
观测空间 = Box(2,)
动作空间 = Discrete(3)
观测范围 = [-1.2  -0.07] ~ [0.6  0.07]
动作数 = 3    
```

强化学习的目标（损失函数）是使总奖励的期望尽可能大。


## Chapter 2. 马尔科夫决策过程(MDP)

