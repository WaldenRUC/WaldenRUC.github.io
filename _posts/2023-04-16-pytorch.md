---
title: 'Pytorch'
date: 2023-04-16
permalink: /posts/2023/04/pytorch/
tags:
  - Pytorch
  - Python
---
# Pytorch学习笔记

## 本文主要记录pytorch中可能经常被遗忘的知识点，包括基本的神经网络设置。主要内容参考[Dive-into-DL-PyTorch](https://github.com/ShusenTang/Dive-into-DL-PyTorch)仓库


### 2-2. tensor

```python
torch.empty(5, 3) #用于创建**未初始化**的Tensor
```

索引形式的结果与**源数据共享内存**，因此修改其中一个，另一个也会修改，例如
```python
a = b.view(15)  # 修改a时，b也会改变
```

pytorch操作inplace版本都有后缀"_"，例如
```python
x.t_()
```

重载运算符:
```python
x += 1 # 在每个x的维度上都加1
z = x + y # 通过广播机制，使得x和y的维度扩充为相同的
y = x ** 2 #也可以写成 y = x * x; element-wise，哈达玛积（点乘）
y = torch.mm(x, x) #矩阵乘，也可以用torch.einsum()
"""
例如: x = [[1, 2]] 大小为(1, 2)，把它扩充到(3, 2)，则结果为
x = [
[1, 2],
[1, 2],
[1, 2]
]
"""
```

为张量创建一个复制而不共享内存:
```python
x_cp = x.clone()
```

tensor与numpy的转换：
numpy()和torch.from_numpy()共用内存;
torch.tensor()不共用内存
```python
b = a.numpy()   # a: tensor; b: numpy
c = torch.from_numpy(d) # c: tensor; d: numpy
b = torch.tensor(a) # a和b不共享内存
```

在GPU上创建张量：
```python
y = torch.ones_like(x, device = torch.device("cuda")    # 创建在GPU上的y
y.to("cpu", torch.double)   # 转移到cpu，保留double格式
```

### 2-3. autograd

每个tensor对象都拥有两个重要属性：
1. .grad属性保存所有.backward()梯度计算之后更新的梯度结果。通过设置.requires_grad=True，后续所有计算都会被记录和追踪。为了防止将来的计算被追踪，可以调用.detach()或者torch.no_grad()，这样未来创建新变量的requires_grad为False
2. .grad_fn为创建该Tensor对象时的function（这个对象可能没有显式地赋予名称）。因此，如果是显式地创建一个tensor，则他的grad_fn应当为None。直接创建的tensor成为梯度图中的叶子结点，可以通过对象的.is_leaf来查询。

注意：每次backward反向传播后，梯度都会累加之前保存的梯度。因此在反向传播前，需要将梯度清零。另外，调用.backward()一般都是标量！

在式子中，部分项detach:

```python
y_1 = x ** 2
with torch.no_grad():
    y_2 = x ** 3    # y_2不记录梯度
y_3 = y_1 + y_2     # y_3包含了y_2项
y_3.backward()      # backward回传之后，x的梯度有何变化？
```
$$
y_3 = y_1 + y_2 = x^2 + x^3
$$
$$
\frac{\partial y_3}{\partial x} = \frac{\partial y_1}{\partial x} + \frac{\partial y_2}{\partial x} = 2x + 3x^2
$$
如果$y_1$被torch.no_grad()，则它对应的梯度$\frac{\partial y_1}{\partial x}$不会回传，因此最终$x$的梯度为$2x$.

如果对象属性requires_grad为False，则不能调用backward()方法，会报错（因为没有grad_fn）

每个tensor都有一个属性.data。这个属性是**不安全的**，因为如果更改了data属性，则此次改变**不会记录在原来的计算图中**，而原来的tensor的值也改变了，因此导数值是错误的。例如：
```python
x = torch.ones(1,requires_grad=True)
y = x ** 2
x.data *= 100 # 只改变了值，不会记录在计算图，所以不会影响梯度传播
print(y)        # 1
y.backward()
print(x) # x = 100; 更改data的值也会影响tensor的值
print(x.grad)   # x.grad = 200; x的梯度为2x，而x在这里已经被修改成100了
```
可见，梯度的计算调用了.data属性，而对data属性的修改是不在计算图内的，因此很危险。

### 3-1 ~ 3-3. linear regression线性回归

生成数据（符合normal分布）：
```python
torch.randn(bsz, feature_dim, dtype=torch.float32)  # 生成一个二维随机标准正态矩阵
np.random.normal(mean, variance, size=(2, 3))   # 生成一个2x3的随机矩阵，数据采样于均值mean，方差variance的正态分布
```

指定torch默认的数据类型:
```python
torch.set_default_tensor_type('torch.FloatTensor')
```

定义Dataset的核心要点：使用Dataset这个子类
```python
from torch.utils.data import Dataset, DataLoader
```
利用Dataset的一个实例dataset，我们可以构造一个DataLoader对象，后续可以通过循环迭代取出一个batch：
```python
data_iter = Data.DataLoader(
    dataset=dataset,    
    batch_size=batch_size,      # mini batch size
    shuffle=True,               # 要不要打乱数据 (打乱比较好)
    num_workers=2,              # 多线程来读数据
)
for batch in data_iter:
    pass
```
我们可以根据自己的数据集格式，手动创建一个Dataset子类，一个简单的例子如下，必须包含__getitem__和__len__两个方法。
```python
class MyDataset(Dataset):
    def __init__(self, myList):
        super(MyDataset, self).__init__()
        self.myList = myList
        self.length = len(myList)
    def __getitem__(self, idx):
        batch = {
            "input_ids": self.myList[idx]
        }
        return batch
    def __len__(self):
        return self.length
```

定义模型的核心要点，它的超集为nn.Module：

```python
class LinearNet(nn.Module):
    def __init__(self, n_feature):
        super(LinearNet, self).__init__()
        self.linear = nn.Linear(n_feature, 1)   # 指定一些网络中的模块，比如线性层

    def forward(self, x):
        # x已经被batch包装好了，这里的大小为[bsz, n_feature]，每个sample的维度即为feature size
        y = self.linear(x)  # 在前向过程中，初始的input作为叶子结点，调用网络模块来计算
        return y
```
也可以用nn.Sequential来定义有序网络与嵌套网络：
```python
class SequentialNet(nn.Sequential):
    pass
```

创建一个模型的实例，例如
```python
net = LinearNet(128)
```
可以通过net.parameters()来查看模型的各个层的参数细节。

### 3-6 ~ 3-7. softmax regression

在网络中套一层其他网络：
```python
class FlattenLayer(nn.Module):
    # 后续可以通过FlattenLayer()来调用
    def __init__(self):
        super(FlattenLayer, self).__init__()
    def forward(self, x): # x shape: (batch, *, *, ...)
        # 将所有的feature拼成一维
        return x.view(x.shape[0], -1)

from collections import OrderedDict
net = nn.Sequential(
    # FlattenLayer(),
    # nn.Linear(num_inputs, num_outputs)
    OrderedDict([
      ('flatten', FlattenLayer()),
      ('linear', nn.Linear(num_inputs, num_outputs))])
    )
```
torch.nn.init可以初始化各层的参数（例如初始化线性层可以传入net.linear.weight或net.linear.bias）
```python
for params in net.parameters():
    init.normal_(params, mean=0, std=0.01)
```

### 3-8 ~ 3-10. mlp

mlp的主要结构如下：
```python
net = nn.Sequential(
    d2l.FlattenLayer(),
    nn.Linear(num_inputs, num_hiddens),
    nn.ReLU(),
    nn.Linear(num_hiddens, num_outputs), 
    )
```

### 3-11 ~ 3-13. 机器学习的其他常见问题
```python
L_2范数惩罚项:
def l2_penalty(w):
    return (w**2).sum() / 2
```
dropout: 丢弃一部分元素，提升泛化能力
torch.zeros_like(X)将X全置0.
(torch.rand(X.shape) < keep_prob).float()获得一个长度为X.shape, 掩码概率为keep_prob的0-1向量。
在实际使用中，调用nn.Dropout即可。
Dropout层输入的参数有一定概率被置零后输出。
```python
net = nn.Sequential(
        d2l.FlattenLayer(),
        nn.Linear(num_inputs, num_hiddens1),
        nn.ReLU(),
        nn.Dropout(drop_prob1),
        nn.Linear(num_hiddens1, num_hiddens2), 
        nn.ReLU(),
        nn.Dropout(drop_prob2),
        nn.Linear(num_hiddens2, 10)
        )
```

### 4-1. 模型构造
isinstance(x, y)用于判断：对象x的类型是否为y。
继承nn.Module的网络类，可以调用self.add_module(k, v)方法，按顺序加入self._modules属性。
一个典型例子：
```python
class MySequential(nn.Module):
    from collections import OrderedDict
    def __init__(self, *args):
        super(MySequential, self).__init__()
        if len(args) == 1 and isinstance(args[0], OrderedDict): # 如果传入的是一个OrderedDict
            for key, module in args[0].items():
                self.add_module(key, module)  # add_module方法会将module添加进self._modules(一个OrderedDict)
        else:  # 传入的是一些Module
            for idx, module in enumerate(args):
                self.add_module(str(idx), module)
    def forward(self, input):
        # self._modules返回一个 OrderedDict，保证会按照成员添加时的顺序遍历成
        for module in self._modules.values():
            input = module(input)
        return input
```
nn.ModuleList()参数为一个列表，列表元素为nn网络类，例如net = nn.ModuleList([nn.Linear(784, 256), nn.ReLU()])。这里不能使用list类来代替，虽然依旧可以在forward函数中计算出结果，但是list内的nn.Module参数不会被注册到网络中，**也就是说，网络无法更新参数，无法训练**。
我个人倾向于用nn.Sequential + OrderedDict来编写网络。

在网络中，可以手动设置一些不可学习（更改）的参数，
```python
self.rand_weight = torch.rand((20, 20), requires_grad=False) # 不可训练参数（常数参数）
```
编写网络的两种方法：
1. 自己定义一个nn.Module的子类，编写__init__和forward函数；
2. 利用nn.Sequential传入一系列已有的模块（如nn.Linear等）作为参数，此时不需要写上述两个函数。
例如，
```python
net = nn.Sequential(NestMLP(), nn.Linear(30, 20), FancyMLP())
X = torch.rand(2, 40)
print(net)
net(X)
```

### 4-2. 模型参数
可以通过net.named_parameters()来访问每个参数。
```python
for name, param in net.named_parameters():
    print(name, param.size())
```
为了使模型注册到网络中，在编写网络时，每个初始化参数tensor需要用nn.Parameter包装：
```python
self.weight1 = nn.Parameter(torch.rand(20, 20))
```
如果需要将参数加入一个列表，同样不能用list，而应当用ParameterList:
```python
self.params = nn.ParameterList([nn.Parameter(torch.randn(4, 4)) for i in range(3)])
```
在forward中，可以用for in的方式抽取出self.params的每一层，例如：
```python

```
此外，初始化参数，可以用init方法，例如将参数变为0，可以调用：
```python
init.constant_(param, val=0)
```
注意：如果**用自定义的方法初始化，**则需要包装torch.no_grad()
共享参数的网络：只需重复引用一个模块实例即可：
```python
linear = nn.Linear(1, 1, bias=False)
net = nn.Sequential(linear, linear) 
```
在Sequential模块中，两个linear实例共享同一套参数。
（共享参数的网络层梯度如何更新？）
```python
x = torch.ones(1, 1)
y = net(x).sum()
print(y)
y.backward()
print(net[0].weight.grad)
## 输出
tensor(9., grad_fn=<SumBackward0>)
tensor([[6.]])
```

### 4-4. 模型层
通过nn.ParameterDict可以接受一个值为nn.Parameter的字典。这个ParameterDict对象也可以如正常字典一样update一个新的ParameterDict，例如：
```python
self.params = nn.ParameterDict({
                'linear1': nn.Parameter(torch.randn(4, 4)),
                'linear2': nn.Parameter(torch.randn(4, 1))
        })
        self.params.update({'linear3': nn.Parameter(torch.randn(4, 2))}) # 新增
```

### 4-5. 读写
torch.save()和torch.load()可以将很多数据结构和存储关联起来，甚至可以操作字典：键依然作为字符串存储，而值转换为tensor。
net.state_dict()可以返回一个有序字典，存储所有层名和对应的参数列表。
注意，如果是用常规方法写，则有序字典的键对应着创建网络时的私有变量名，例如：
```python
def __init__(self):
    super(MLP, self).__init__()
    self.hidden = nn.Linear(3, 2)
    self.act = nn.ReLU()
    self.output = nn.Linear(2, 1)
```
就对应着以下state_dict():
```python

OrderedDict([('hidden.weight', tensor([[ 0.1836, -0.1812, -0.1681],
                      [ 0.0406,  0.3061,  0.4599]])),
             ('hidden.bias', tensor([-0.3384,  0.1910])),
             ('output.weight', tensor([[0.0380, 0.4919]])),
             ('output.bias', tensor([0.1451]))])
```
同理，optimizer也可以调用state_dict()，但返回值包含了更多信息，诸如learning rate和momentum等等。
**存储模型，等同于存储模型的state_dict()**。因此，存储模型应当这样使用：
```python
torch.save(net.state_dict(), PATH)
```

### 4-6. 使用GPU
将一个tensor从cpu转换到cuda:0上：
```python
x = x.cuda(0)
```
返回结果：tensor([1,2,3], device='cuda:0')
注意，这里创建一个GPU版本的x，需要利用返回值重新赋值。
相反，对于模型而言，
```python
net.cuda()
```
直接调用.cuda()方法，net自身就迁移到GPU上了。

### 5-1 ~ 5-4. CNN基础知识

CNN就是通过一个固定的小核，将一个矩阵变换到另一个大小的矩阵。
输入为X（矩阵）和K（小核）。
```python
def corr2d(X, K):  # 本函数已保存在d2lzh_pytorch包中方便以后使用
    h, w = K.shape  # Kernel的大小为h*w
    X, K = X.float(), K.float() 
    Y = torch.zeros((X.shape[0] - h + 1, X.shape[1] - w + 1)) # 最终的输出大小
    for i in range(Y.shape[0]):
        for j in range(Y.shape[1]):
            Y[i, j] = (X[i: i + h, j: j + w] * K).sum() # 输出的每个元素为原始X的子矩阵
    return Y
```

CNN中，输入X的前两维分别为**批量大小**和**通道数**。输入与输出的批量大小不变，而通道数可能发生变化，torch.nn.Conv2d的主要参数如下：

- in_channels: 输入张量的通道数
- out_channels: 输出张量的通道数
- kernel_size: 卷积核的大小（二维），设大小为$(k_1, k_2)$
- padding: 在输出张量最后两维两侧分别填充的大小，设大小为$(p_1, p_2)$
- stride: 在卷积之前，对原矩阵填充边界，设大小为$(s_1, s_2)$

设输入X的矩阵大小（不考虑批量和通道）为(8, 8)，则通过卷积核输出的大小为(8-(a-1), 8-(b-1))。再通过填充层厚，大小变为(8-(a-1)+2c, 8-(b-1)+2d)。
